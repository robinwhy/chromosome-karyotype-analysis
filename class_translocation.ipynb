{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371 files, 1 folders\n"
     ]
    }
   ],
   "source": [
    "import os,shutil\n",
    "files = folders = 0\n",
    "for _, dirnames, filenames in os.walk(\"../anomaly_karyotype/\"):\n",
    "  # ^ this idiom means \"we won't be using this value\"\n",
    "    files += len(filenames)\n",
    "    folders += len(dirnames)\n",
    "print (\"{:,} files, {:,} folders\".format(files, folders))\n",
    "#around 180 abnormal folders, use 20 normal folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "dirpath=\"../anomaly_karyotype/translocation_gan/\"\n",
    "x=[]\n",
    "img = data.imread(dirpath+\"1.png\")\n",
    "print(img.shape)\n",
    "# print(type(X_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(\"../rawdata.zip\", 'zip', \"../rawdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"../anomaly_karyotype.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"../anomaly_karyotype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Reshape,Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import hamming_loss\n",
    "from keras import backend as K\n",
    "import os,random,shutil\n",
    "from skimage import io,color,data,measure,morphology\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.measure import label, regionprops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660\n",
      "660 74 1540\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "image_path = \"../anomaly_karyotype/translocation_single/\"     # images\n",
    "normal_path=\"../anomaly_karyotype/normal_single/\"\n",
    "y=[]\n",
    "x = []\n",
    "count=0\n",
    "for _, dirnames, filenames in os.walk(\"../anomaly_karyotype/translocation_single\",topdown=False):\n",
    "    for d in dirnames:\n",
    "        folder = image_path + d+'/'\n",
    "        for _, dirnames2, filenames2 in os.walk(folder,topdown=False):\n",
    "            for d in dirnames2:\n",
    "                subfolder=folder+d+'/'\n",
    "                if os.path.isdir(subfolder):\n",
    "                    images = [f for f in os.listdir(subfolder) if os.path.isfile(os.path.join(subfolder, f))]\n",
    "                    for j in images:\n",
    "                        img = data.imread(os.path.join(subfolder,j),as_gray=True)\n",
    "                        try:\n",
    "                            img = resize(img,(128,128,3))\n",
    "                        except BaseException:\n",
    "                            print(os.path.join(subfolder, j))\n",
    "                        if j[-5]=='t':\n",
    "                            img2=np.fliplr(img)\n",
    "                            y.append(1)\n",
    "                            y.append(1)\n",
    "                            x.append(img)\n",
    "                            x.append(img2)\n",
    "#                             io.imsave(\"../anomaly_karyotype/translocation_gan/\"+str(count)+\".png\",img)\n",
    "#                             count+=1\n",
    "random.shuffle(x)\n",
    "xtest=x[-74:]\n",
    "ytest=y[-74:]\n",
    "x=x[:-74]\n",
    "y=y[:-74]\n",
    "print(len(x))\n",
    "# dirpath=\"../anomaly_karyotype/gan_filtered\"\n",
    "# for _, dirnames, filenames in os.walk(\"../anomaly_karyotype/gan_filtered\",topdown=False):\n",
    "#             images = [os.path.join(dirpath, f) for f in filenames if os.path.isfile(os.path.join(dirpath, f))]\n",
    "#             for j in images:\n",
    "#                 img = data.imread(j,as_gray=True)\n",
    "#                 img = resize(img,(128,128,3))\n",
    "#                 x.append(img)\n",
    "#                 y.append(1)\n",
    "normal=[]#35 normal folders, 16 for training, 19 for test\n",
    "for _, dirnames, filenames in os.walk(\"../anomaly_karyotype/normal_single\",topdown=False):\n",
    "    for d in dirnames:\n",
    "        folder = normal_path + d+'/'\n",
    "        if os.path.isdir(folder):\n",
    "            images = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "            for j in images:\n",
    "                img = data.imread(os.path.join(folder,j),as_gray=True)\n",
    "                try:\n",
    "                    img = resize(img,(128,128,3))\n",
    "                except BaseException:\n",
    "                    print(os.path.join(folder, j))\n",
    "                normal.append(img)\n",
    "                img2=np.fliplr(img)\n",
    "                normal.append(img2)\n",
    "print(len(y),len(ytest),len(normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_normal=normal[1140:]\n",
    "ytest_normal=[]\n",
    "x=x+normal[:1140]\n",
    "for i in range(1140):\n",
    "    y.append(0)\n",
    "for i in range(400):\n",
    "    ytest_normal.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_normal=normal[660:]\n",
    "ytest_normal=[]\n",
    "x=x+normal[:660]\n",
    "for i in range(660):\n",
    "    y.append(0)\n",
    "for i in range(880):\n",
    "    ytest_normal.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1320, 128, 128, 3) (1320,) (74, 128, 128, 3) (880, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(x)\n",
    "y_train=np.array(y)\n",
    "x_test=np.array(xtest)\n",
    "y_test=np.array(ytest)\n",
    "x_test_normal=np.array(xtest_normal)\n",
    "y_test_normal=np.array(ytest_normal)\n",
    "indices = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]\n",
    "# indices = np.arange(x_test.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# x_test = x_test[indices]\n",
    "# y_test = y_test[indices]\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "x_test_normal= x_test_normal.astype('float32')\n",
    "print(x_train.shape,y_train.shape,x_test.shape,x_test_normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hywang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "base_model=VGG19(weights= None, include_top=False, input_shape= (128,128,3))\n",
    "x = base_model.output\n",
    "x=Flatten()(x)\n",
    "predictions = Dense(2, activation= 'softmax')(x)\n",
    "model = Model(inputs = base_model.input, outputs = predictions)\n",
    "adam=Adam(lr=0.0001)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hywang/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1188 samples, validate on 132 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# checkpoint = ModelCheckpoint('dense121_0801.h5', monitor='val_loss',verbose=1,save_best_only=True,mode='min')\n",
    "# history=model.fit(x_train, y_train,batch_size=32,epochs=10,validation_split=0.1,callbacks = [checkpoint])\n",
    "history=model.fit(x_train, y_train,batch_size=32,epochs=10,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train, y_train,batch_size=32,epochs=5,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# model.load_weights(\"dense201.h5\")\n",
    "y_pred = model.predict(x_test,batch_size=16, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_normal,batch_size=16, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test_normal, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_for_tensorflow",
   "language": "python",
   "name": "kernel_for_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
